{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP 1 : Download all necessary tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import Env\n",
    "from gym.spaces import Box, Discrete\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from collections import deque\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import pytesseract\n",
    "import pydirectinput\n",
    "from matplotlib import pyplot as plt\n",
    "from mss import mss\n",
    "import pyautogui\n",
    "import time\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuração do ambiente\n",
    "# env = gym.make('CartPole-v1')\n",
    "# state_size = env.observation_space.shape[0]\n",
    "# action_size = env.action_space.n\n",
    "\n",
    "# Parâmetros para o DQN\n",
    "learning_rate = 0.001\n",
    "discount_rate = 0.95\n",
    "exploration_rate = 1.0\n",
    "max_exploration_rate = 1.0\n",
    "min_exploration_rate = 0.01\n",
    "exploration_decay_rate = 0.995\n",
    "total_episodes = 1000\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Verificar se a GPU está disponível\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DinoAI(Env):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.observation_space = Box(low = 0, high = 255, shape = (1,83,100), dtype = np.uint8)\n",
    "        self.action_space = Discrete(3)\n",
    "        #Capture enviromment\n",
    "        self.cap = mss()\n",
    "        self.game_location = {'top':350, 'left': 0,'width':250, 'height': 150}\n",
    "        self.gameOver_location = {'top':330, 'left': 310,'width':330, 'height': 25}\n",
    "     \n",
    "    def set_env(self):\n",
    "        # Pressiona a tecla Windows\n",
    "        pyautogui.press('win')\n",
    "        time.sleep(1)\n",
    "        # Escreve \"Google Chrome\"\n",
    "        pyautogui.write('Google Chrome')\n",
    "        time.sleep(1)\n",
    "        # Aperta Enter\n",
    "        pyautogui.press('enter')\n",
    "        time.sleep(2)\n",
    "        pyautogui.hotkey('ctrl', 'l')\n",
    "        time.sleep(1)\n",
    "        # Escreve \"chrome://dino\" na URL\n",
    "        pyautogui.write('chrome://dino')\n",
    "        pyautogui.press('enter')\n",
    "        time.sleep(1)\n",
    "        # Pressiona \"Windows + seta para a esquerda\"\n",
    "        pyautogui.hotkey('win', 'left')\n",
    "        time.sleep(1)\n",
    "        pyautogui.hotkey('alt', 'tab')\n",
    "        time.sleep(1)\n",
    "        pyautogui.hotkey('win', 'right')\n",
    "        time.sleep(1)\n",
    "        pyautogui.hotkey('alt', 'tab')\n",
    "        time.sleep(1)\n",
    "        pyautogui.press('space')\n",
    "        time.sleep(2)\n",
    "\n",
    "    def get_gameOver(self):\n",
    "\n",
    "        #time.sleep(5)\n",
    "        gameOver_snip = np.array(self.cap.grab(self.gameOver_location))[:,:,:3]\n",
    "        gray_gameOver = cv2.cvtColor(gameOver_snip, cv2.COLOR_BGR2GRAY)\n",
    "        #print(\"forma:\",gray_gameOver)\n",
    "\n",
    "        return gray_gameOver\n",
    "\n",
    "    def get_observation(self):\n",
    "        #Start to set the enviroment\n",
    "        #self.set_env()\n",
    "        game_snip = np.array(self.cap.grab(self.game_location))[:,:,:3]\n",
    "        gray = cv2.cvtColor(game_snip, cv2.COLOR_BGR2GRAY) \n",
    "        resized = cv2.resize(gray, (100,83))\n",
    "        channel = np.reshape(resized, (1,83,100))\n",
    "\n",
    "        return channel\n",
    "    \n",
    "    def get_done(self):\n",
    "\n",
    "        gameOver_string = ['GAME','GAHE']\n",
    "        done = False\n",
    "        res = pytesseract.image_to_string(self.get_gameOver())[:4]\n",
    "        if res in gameOver_string:\n",
    "            done = True\n",
    "\n",
    "        return done\n",
    "    \n",
    "\n",
    "    def step(self,action):\n",
    "\n",
    "        if action == 1 :\n",
    "            pyautogui.press('space')\n",
    "        elif action == 2:\n",
    "            pyautogui.press('down')\n",
    "\n",
    "        new_state = self.get_observation()\n",
    "\n",
    "        done = self.get_Done()\n",
    "\n",
    "        reward = 1\n",
    "\n",
    "        info = {}\n",
    "\n",
    "        return new_state, reward, done, info\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def play_game(self):\n",
    "        #self.set_env()\n",
    "        done = False\n",
    "        while(not done):\n",
    "            new_state, reward, done, _ = self.step()\n",
    "\n",
    "            print(done)\n",
    "        \n",
    "        \n",
    "    def reset(self):\n",
    "        pyautogui.press('space')\n",
    "\n",
    "\n",
    "    # def render(self):\n",
    "    #     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = DinoAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rede neural para Deep Q-learning Model\n",
    "# class DQN(nn.Module):\n",
    "#     def __init__(self, state_size, action_size = 3):\n",
    "#         super(DQN, self).__init__()\n",
    "#         self.fc1 = nn.Linear(state_size, 24)\n",
    "#         self.fc2 = nn.Linear(24, 24)\n",
    "#         self.fc3 = nn.Linear(24, action_size)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         x = torch.relu(self.fc1(x))\n",
    "#         x = torch.relu(self.fc2(x))\n",
    "#         x = self.fc3(x)\n",
    "#         return x\n",
    "#     # Função para escolher a ação\n",
    "\n",
    "\n",
    "def conv2d_size_out(size, kernel_size=5, stride=2):\n",
    "    return (size - (kernel_size - 1) - 1) // stride + 1\n",
    "\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, img_height, img_width, num_actions = 3):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=2)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=5, stride=2)\n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(img_width)))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(img_height)))\n",
    "        linear_input_size = convw * convh * 32\n",
    "        self.fc1 = nn.Linear(linear_input_size, 256)\n",
    "        self.fc2 = nn.Linear(256, num_actions)  # Saída tem dimensão de num_actions\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)  # Flatten da saída da última camada convolucional\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to create tensor with negative dimension -449664: [256, -449664]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [44], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m state_size \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mgame_location[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwidth\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m env\u001b[38;5;241m.\u001b[39mgame_location[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheight\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      2\u001b[0m action_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mDQN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_size\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# Mover o modelo para a GPU\u001b[39;00m\n\u001b[0;32m      4\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[0;32m      5\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()\n",
      "Cell \u001b[1;32mIn [43], line 30\u001b[0m, in \u001b[0;36mDQN.__init__\u001b[1;34m(self, img_height, img_width, num_actions)\u001b[0m\n\u001b[0;32m     28\u001b[0m convh \u001b[38;5;241m=\u001b[39m conv2d_size_out(conv2d_size_out(conv2d_size_out(img_height)))\n\u001b[0;32m     29\u001b[0m linear_input_size \u001b[38;5;241m=\u001b[39m convw \u001b[38;5;241m*\u001b[39m convh \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m32\u001b[39m\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1 \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlinear_input_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m256\u001b[39m, num_actions)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\linear.py:96\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[1;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_features \u001b[38;5;241m=\u001b[39m in_features\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_features \u001b[38;5;241m=\u001b[39m out_features\n\u001b[1;32m---> 96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m Parameter(torch\u001b[38;5;241m.\u001b[39mempty((out_features, in_features), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs))\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bias:\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;241m=\u001b[39m Parameter(torch\u001b[38;5;241m.\u001b[39mempty(out_features, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Trying to create tensor with negative dimension -449664: [256, -449664]"
     ]
    }
   ],
   "source": [
    "state_size = env.game_location['width'] * env.game_location['height']\n",
    "action_size = 3\n",
    "model = DQN(state_size, action_size).to(device)  # Mover o modelo para a GPU\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "memory = deque(maxlen=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state, exploration_rate):\n",
    "    if np.random.rand() <= exploration_rate:\n",
    "        return random.randrange(action_size)\n",
    "    state = torch.FloatTensor(state).to(device)\n",
    "    with torch.no_grad():\n",
    "        q_values = model(state)\n",
    "    return np.argmax(q_values.cpu().numpy())\n",
    "\n",
    "# Função para treinar o modelo\n",
    "def train_model(batch_size):\n",
    "    minibatch = random.sample(memory, batch_size)\n",
    "    for state, action, reward, next_state, done in minibatch:\n",
    "        state = torch.FloatTensor(state).to(device)\n",
    "        next_state = torch.FloatTensor(next_state).to(device)\n",
    "        reward = torch.FloatTensor([reward]).to(device)\n",
    "        done = torch.FloatTensor([done]).to(device)\n",
    "\n",
    "        target = reward\n",
    "        if not done:\n",
    "            target = reward + discount_rate * torch.max(model(next_state))\n",
    "\n",
    "        target_f = model(state)\n",
    "        target_f = target_f.clone()\n",
    "        target_f[0][action] = target\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(model(state), target_f)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loop principal de treinamento\n",
    "for e in range(total_episodes):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    for time in range(500):\n",
    "        action = choose_action(state, exploration_rate)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        memory.append((state, action, reward, next_state, done))\n",
    "        state = next_state\n",
    "        if done:\n",
    "            print(f\"Episode: {e+1}/{total_episodes}, Score: {time}, Exploration rate: {exploration_rate:.2f}\")\n",
    "            break\n",
    "        if len(memory) > batch_size:\n",
    "            train_model(batch_size)\n",
    "    exploration_rate *= exploration_decay_rate\n",
    "    exploration_rate = max(min_exploration_rate, exploration_rate)\n",
    "\n",
    "# Salvar o modelo\n",
    "torch.save(model.state_dict(), 'dqn_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
